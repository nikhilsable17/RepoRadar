{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPB+3uySu6ZZjzRSmI55cvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilsable17/RepoRadar/blob/main/gitScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RepoRadar:** Unveiling GitHub's Top Repositories"
      ],
      "metadata": {
        "id": "_tztQz0TfRxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivation behind working on this project: -\n",
        "\n",
        "In light of the ever-expanding GitHub repository ecosystem and the inherent challenge of navigating and extracting valuable insights from this vast code repository platform, I found a compelling need to develop a systematic web scraping solution. Recognizing the lack of readily available tools to comprehensively capture and organize information about top repositories under different topics, I decided to embark on a project that would leverage Python libraries like Beautiful Soup, Pandas, and requests. The goal is to create an efficient and accessible means of exploring GitHub's diverse coding landscape, allowing users to uncover hidden gems and trending projects within specific topics. This decision was motivated by the desire to empower developers and researchers with a user-friendly tool for informed decision-making and exploration in the GitHub ecosystem."
      ],
      "metadata": {
        "id": "F47KCl7dfmOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps I  followed here: -\n",
        "* We'll be Scraping https://github.com/topics\n",
        "\n",
        "* We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
        "\n",
        "* For each topic, we'll get the top repositories in the topic from the topic page\n",
        "\n",
        "* For each repository, we'll grab the repo name, username, stars and repo URL\n",
        "\n",
        "* For each topic we'll create a .csv file in the following format:\n",
        "\n",
        "  Repo Name,Username,Stars,Repo URL\n",
        "\n",
        "  three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
        "\n",
        "  libgdx,libgdx,18300,https://github.com/libgdx/libgdx"
      ],
      "metadata": {
        "id": "PCTQSzRhi1Qa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vE5pwJHmzn-"
      },
      "outputs": [],
      "source": [
        "pip install requests --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4 --upgrade --quiet"
      ],
      "metadata": {
        "id": "sdwCqnxhnGxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas --quiet"
      ],
      "metadata": {
        "id": "Fe7Z7J35nJ5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some of my Helping Hands!\n",
        "\n",
        "- **Requests: -** Utilize the requests library to effortlessly fetch data from the web, which makes our project a master communicator with online sources.\n",
        "\n",
        "- **BeautifulSoup: -** Let beautifulsoup weave its magic as it parses HTML and XML documents, simplifying the extraction of valuable data from the web.\n",
        "\n",
        "- **Regex (re): -** Employ regular expressions (re) to pinpoint and extract precisely the information we need, adding a touch of surgical precision to our data extraction.\n",
        "\n",
        "- **Pandas: -** Embrace the data manipulation capabilities of pandas to organize, clean, and analyze the extracted data seamlessly. Transform messy data into structured datasets with ease.\n",
        "\n",
        "- **os: -** Leveraging the os library to manage our project's file system, providing a robust foundation for organizing and storing our scraped data efficiently"
      ],
      "metadata": {
        "id": "0DvngKCUmbeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "31TN35ZqnTqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping all the Topic data here: -\n",
        "- Scraped all the Topic Title\n",
        "  * The code goes through all the Parent tags from the extracted HTML page and stores inside a variable\n",
        "  \n",
        "  * Iterating to the length of the above variable, we append all the Titles in an empty list.\n",
        "\n",
        "- Topic Description & Title URL's: -\n",
        "  * We follow the same process for the remaining functions as mention for this cell!\n",
        "\n",
        "  * We use a .strip() to strip, if any whitespaces or any specificed characters from the START / END of the string\n"
      ],
      "metadata": {
        "id": "uHHhALQ9tTzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Scrape all the Topic Titles\n",
        "def get_topic_titles(doc):\n",
        "    try:\n",
        "        t_titles = doc.find_all('p',{'class','f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
        "        topic_titles = []\n",
        "        for i in t_titles:\n",
        "            topic_titles.append(i.text)\n",
        "        return topic_titles\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "# Function to Scrape all the Topic Description\n",
        "def get_topic_desc(doc):\n",
        "    try:\n",
        "        t_titles_desc = doc.find_all('p', {'class', 'f5 color-fg-muted mb-0 mt-1'})[:]\n",
        "        topic_title_desc_1 = []\n",
        "        for i in t_titles_desc:\n",
        "            topic_title_desc_1.append(i.text.strip())\n",
        "            return topic_title_desc_1\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "# Function to Scrape all the Topic URL's\n",
        "def get_topic_urls(doc):\n",
        "    try:\n",
        "        big_box_topics = doc.find_all('div',{'class', 'py-4 border-bottom d-flex flex-justify-between'})\n",
        "        #box_topics = big_box_topics[0]\n",
        "\n",
        "        #url = box_topics.a['href']\n",
        "        topic_urls = []\n",
        "        for i in big_box_topics:\n",
        "            topic_urls.append(\"https://github.com\" + i.a['href'])\n",
        "        return topic_urls\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def scrape_topics():\n",
        "    try:\n",
        "        topics_url= 'https://github/.com/topics'\n",
        "        topic_page = requests.get(topics_url)\n",
        "        if topic_page.status_code != 200:\n",
        "            raise Exception ('Error while Loading Topic Page: - {}'.format(Exception))\n",
        "        doc = bs(topic_page.text, 'html.parser')\n",
        "        topics_dict = {'Titles': get_topic_titles(doc),\n",
        "                    'Description' : get_topic_desc(doc),\n",
        "                    'Urls': get_topic_urls(doc)}\n",
        "        return pd.DataFrame(topics_dict)\n",
        "    except Exception as e:\n",
        "        print(e)\n"
      ],
      "metadata": {
        "id": "PZNOWJh8ncjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping the Repo Data: -\n",
        "\n",
        "- Here we first call the (get_topic_repos)function where we create a variable to store all the parent tags, after which we create a disctionary in which we store the Scraped record!\n",
        "\n",
        "- Then we pass the whole html corpus, itrating to the length of all Parent tags, where the function calls (get_repo_data) & Scrape the required data one by one!\n",
        "\n",
        "- After Scraping the data function returns the data to the disctionary, & the disctionary is then passed to create a Data Frame"
      ],
      "metadata": {
        "id": "ZaqUvnr65HZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function for Scraping data from the repository for a perticular topic\n",
        "def get_repo_data(repo,i):\n",
        "    try:\n",
        "        username = repo[i].div.find_all('a',{'class':'Link'})[0].text.strip()\n",
        "        reposatory_name = repo[i].div.find_all(\"a\",{'class', 'Link text-bold wb-break-word'})[0].text.strip()\n",
        "        star_repo = repo[i].find_all(\"span\",{'class', 'Counter js-social-count'})[0].text\n",
        "        reposatory_url = \"https://github.com\" + repo[i].find_all(\"a\",{'class', 'Link'})[1]['href']\n",
        "        stars = star_conversion(star_repo)\n",
        "        return username, reposatory_name, reposatory_url, stars #star_repo\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def get_topic_repos(topic_page_html): # we have got the entire html junk here topic_page_html from def get_topic_page(topic_url)\n",
        "    repo_big_box = topic_page_html.find_all(\"article\", {\"class\" : \"border rounded color-shadow-small color-bg-subtle my-4\"})\n",
        "\n",
        "    reposatory_dict = {\"Username\":[], \"Reposatory_Name\":[], \"Stars\":[], \"Reposatory_URL's\":[]}\n",
        "\n",
        "    for i in range(len(repo_big_box)):\n",
        "        repo_info = get_repo_data(topic_page_html,i) # repo_html_1,topic_page_html\n",
        "        reposatory_dict['Username'].append(repo_info[0])\n",
        "        reposatory_dict['Reposatory_Name'].append(repo_info[1])\n",
        "        reposatory_dict['Stars'].append(repo_info[2])\n",
        "        reposatory_dict[\"Reposatory_URL's\"].append(repo_info[3])\n",
        "\n",
        "    return pd.DataFrame(reposatory_dict)"
      ],
      "metadata": {
        "id": "4kTDuOaMrnkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating all the Web page into a Beautified HTML coupus!\n",
        "\n",
        "- Here the Base link i.e \"https://github.com/topics\" is passed to request library where .get() sends a GET request to the web server (in simple words, the server requests a website to open the URL).\n",
        "\n",
        "- A successfull response will return a Response200 ! return code which simple means We have successfully hit the requested site!\n",
        "\n",
        "- We have also took care if anything went in a unexpected way!\n",
        "\n",
        "- Then we store whole HTML corpus in a way better Beautified way by passing it **\"Beautiful Soup\"**  & store it to a variable!\n",
        "\n",
        "- We have made an another function where we convert the Rating in \"...k\" to \"..000\" **\"eg: - 97k to 97000\"** which will help the end user to easily understand!"
      ],
      "metadata": {
        "id": "vuCY8mEkhkl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for getting & loading the page in html\n",
        "def get_topic_page(topic_url):\n",
        "    #Loading the topic page: -\n",
        "    topic_page = requests.get(topic_url)\n",
        "\n",
        "    #checking for successfull execution of the topic page\n",
        "    if topic_page.status_code != 200:\n",
        "        raise Exception ('Error while Loading Topic Page: - {}'.format(topic_url))\n",
        "\n",
        "    # Beautifying using bs\n",
        "    topic_page_html = bs(topic_page.text, 'html.parser')\n",
        "\n",
        "    return topic_page_html\n",
        "\n",
        "\n",
        "# Function Creation for Converting ***k starts to integer values: -\n",
        "def star_conversion(star_str):\n",
        "    star_str = star_str.strip()\n",
        "    if star_str[-1] == 'k':\n",
        "        return int(float(star_str[:-1]) * 1000)\n",
        "    return int(star_str)\n",
        "\n"
      ],
      "metadata": {
        "id": "xhTswFIFnWH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can call it as 2nd Base function: -\n",
        "- Firstly we check for the file name(w.r.t. the File path), if we get any identical file name then it automatically skip & continue for the rest!\n",
        "\n",
        "\n",
        "- All the Scraping process starts here\n",
        "\n",
        "- Here we Firstly call the get_topic_function to Scrape all the Topic info.\n",
        "\n",
        "- Then the resultant value is passed to Scrape the Repos for the respective Topic\n",
        "\n",
        "- And finally we are available with a CSV file named with the respective Topic Title name!"
      ],
      "metadata": {
        "id": "BzueyX_VuoD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topic(topic_url, path): #filename = topic_titles + '.csv'\n",
        "    try:\n",
        "        if os.path.exist(path):\n",
        "            print(\"File {} Already Exists\".format(path))\n",
        "            return\n",
        "\n",
        "        topic_df_1 = get_topic_repos(get_topic_page(topic_url))\n",
        "        topic_df_1.to_csv(path,index = None)\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "5syESv8knfFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Base Function (Main Function)\n",
        "\n",
        "- We call the Base function 2, in which all the Scraping process.\n",
        "\n",
        "- When all the information is Scraped from the above func, here we create a Folder named \"gitHub_Scrape\" where all the Scraped CSV files w.r.t \"Topic Title\" having the top repositories w.r.t Stars are grouped here!"
      ],
      "metadata": {
        "id": "nWN6duqSx8vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base function to start with\n",
        "def scrape_topics_repos():\n",
        "    topics_df = scrape_topics()\n",
        "\n",
        "    # Creating a Folder for all the Scraped data (CSV files)\n",
        "    os.makedirs(\"gitHub_Scrape\", exist_ok = True)\n",
        "\n",
        "    for row in topics_df.iterrows(): #index,\n",
        "        print(\"Scraping top reposatories for '{}'\".format(row['Title']))\n",
        "        scrape_topic(row['URLs'], \"gitHub_Scrape/{}.csv\".format(row['Titles']))"
      ],
      "metadata": {
        "id": "KPco-wI9ng3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call this Function"
      ],
      "metadata": {
        "id": "0_8dJYK-zZQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_topics_repos()"
      ],
      "metadata": {
        "id": "YeNmfqqmqfNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References & Future Work: -\n",
        "\n",
        "* Summary: -\n",
        "\n",
        " * In this web scraping project, I utilized Python's Beautiful Soup, Pandas, requests, and os libraries to extract valuable information from GitHub's website. The primary goal was to navigate through the Topics and gather data from all the available topics.\n",
        "\n",
        " * Scraping process involved extracting data such as the\n",
        "   * Topic Name\n",
        "   \n",
        "   * Topic Description\n",
        "\n",
        "   * Topic URL\n",
        "\n",
        "   * Repository Title\n",
        "\n",
        "   * Name of the Author\n",
        "\n",
        "   * Repository URL\n",
        "\n",
        "   * Rating (Stars)\n",
        "\n",
        "  This comprehensive approach allowed me to capture a holistic view of the GitHub repositories under each topic.\n",
        "\n",
        " * Marshallizing the data effectively, I created separate .CSV files for each topic, containing information about the top repositories based on their Star Rating. I also make user that No file is repeated with the Same Title/ File Name.\n",
        "\n",
        " * Additionally, a dedicated folder was Created to store all the CSV files, ensuring a structured and easily accessible repository of information.\n",
        "\n",
        " * This project not only showcased proficiency in web scraping using Python but also demonstrated the ability to efficiently manage and present the extracted data. The resulting CSV files provide a quick reference to the top repositories in each topic, offering valuable insights for further analysis or exploration."
      ],
      "metadata": {
        "id": "_qTfIDz0PW1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References to link that found useful\n",
        "\n",
        "* https://pandas.pydata.org/\n",
        "\n",
        "* https://pypi.org/project/beautifulsoup4/\n",
        "\n",
        "* https://docs.python.org/3/library/os.html\n",
        "\n",
        "* https://python.readthedocs.io/en/v2.7.2/library/os.html\n",
        "\n",
        "* https://pypi.org/project/requests/\n",
        "\n",
        "* https://docs.python.org/3/library/re.html\n",
        "\n",
        "* https://stackoverflow.com/"
      ],
      "metadata": {
        "id": "NPW2x717Z1Nx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideas for Future Work: -\n",
        "Due to some limited timeframe & time constraints, certain aspects were prioritized. Moving forward, with additional time and resources, there are numerous avenues to explore and enhance the project further. Here are some potential areas for improvement and expansion: -\n",
        "\n",
        "* Dynamic Data Updates\n",
        "\n",
        "* Dynamic Data Updates:\n",
        "\n",
        "* Extended Metadata Extraction\n",
        "\n",
        "* Sentiment Analysis on Comments\n",
        "\n",
        "* Machine Learning Models\n",
        "\n",
        "* Cross-Platform Integration\n",
        "\n",
        "* Automated Email Alerts\n",
        "\n",
        "* Integration with Version Control Systems\n",
        "\n",
        "* Security Analysis\n",
        "\n",
        "These ideas provide a foundation for future work that can enhance the functionality, usability, and insights derived from your GitHub web scraping project"
      ],
      "metadata": {
        "id": "YC7KJKchcSuZ"
      }
    }
  ]
}